# ğŸ™ GITHUB README â€” LLaMA 3.2 Vision Chest X-Ray Fine-Tuning

```markdown
# ğŸ« Towards a Foundation Model for Chest X-Ray Interpretation

### LLaMA 3.2 Vision Fine-Tuning on Medical Imaging with Unsloth

<p align="center">
  <img src="https://img.shields.io/badge/LLaMA-3.2%20Vision-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Unsloth-2x%20Faster-orange?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/LoRA%2FQLoRA-PEFT-purple?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Task-Medical%20Imaging-red?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/License-MIT-brightgreen?style=for-the-badge"/>
</p>

<p align="center">
  <b>A complete fine-tuning pipeline for adapting LLaMA 3.2 Vision to chest X-ray classification,
  captioning, and report generation â€” using LoRA/QLoRA on a single GPU.</b>
</p>

---

<img width="1536" height="1024" alt="ChatGPT Image Sep 27, 2025, 02_37_15 PM" src="https://github.com/user-attachments/assets/6798a884-f838-43d0-8c6d-dd9c365eee7d" />


## ğŸ¯ Overview

This repository demonstrates how to fine-tune **Meta's LLaMA 3.2 Vision** on chest X-ray datasets for three clinical tasks:

- ğŸ” **Classification** â€” Identify pathologies (Pneumonia, Effusion, Atelectasis, Normal, etc.)
- ğŸ“ **Report Generation** â€” Produce radiology-style findings descriptions
- ğŸ’¬ **Visual Question Answering (VQA)** â€” Answer clinical questions about X-ray findings

Using **Unsloth**, training is **2Ã— faster** with **60% less VRAM** than standard fine-tuning â€”
making this accessible on a single consumer or research GPU.

---

## ğŸŒ Motivation

> There are over **2 billion chest X-rays** performed globally every year.

Radiologist shortages create dangerous delays â€” particularly in low-resource settings.
Vision-Language Models fine-tuned on medical imaging data offer a path toward:

- Automated first-pass report generation for radiologist review
- High-priority finding flagging for faster clinical triage
- Diagnostic support in under-resourced healthcare systems
- Medical education and training assistance

**This project is a research demonstration â€” not a clinical product.**

---

## âœ¨ Features

- âœ… **LLaMA 3.2 Vision fine-tuning** with Unsloth for efficient multimodal training
- âœ… **LoRA / QLoRA** â€” adapt an 11B model on a single GPU
- âœ… **Medical dataset integration** â€” ChestX-ray14, MIMIC-CXR, or custom datasets
- âœ… **Three task modes** â€” classification, captioning, visual QA
- âœ… **Evaluation metrics** â€” Accuracy, BLEU, ROUGE
- âœ… **End-to-end inference** â€” raw X-ray image to clinical text output

---

## ğŸ§  Pipeline

```
Chest X-Ray Image
      â†“
LLaMA 3.2 Vision Encoder (frozen)
      â†“
Cross-Modal Attention â€” Image + Text
      â†“
LoRA-adapted Language Model Head
      â†“
Clinical Text Output
  â†’ "No acute cardiopulmonary findings."
  â†’ "Right lower lobe pneumonia. Clinical correlation recommended."
  â†’ "Large left pleural effusion. Urgent evaluation advised."
```

**LoRA Config:**
- Rank r=16 | Target: q_proj, v_proj, k_proj, o_proj
- 4-bit QLoRA quantization
- ~1â€“2% trainable parameters of total model

---

## ğŸ“Š Example Outputs

| X-Ray Finding | Model Output |
|---------------|-------------|
| Normal PA film | âœ… "Normal chest X-ray. No acute findings identified." |
| Lower lobe opacity | âš ï¸ "Right lower lobe consolidation consistent with pneumonia." |
| Left fluid collection | ğŸ”´ "Large left pleural effusion with compressive atelectasis." |
| Enlarged heart | âš ï¸ "Increased cardiac silhouette suggestive of cardiomegaly." |

---

## ğŸš€ Quick Start

```bash
# 1. Clone
git clone https://github.com/your-username/llm-chest-xray.git
cd llm-chest-xray

# 2. Install
pip install unsloth transformers datasets accelerate peft bitsandbytes

# 3. Open notebook
jupyter notebook Llama_3_2_Vision_Finetuning_Unsloth_Xrays.ipynb
```

**Dataset format:**
```python
{
  "image": "path/to/xray.jpg",
  "label": "Pneumonia",
  "report": "Findings suggest right lower lobe consolidation..."
}
```

Supported datasets: [NIH ChestX-ray14](https://nihcc.app.box.com/v/ChestXray-NIHCC) Â· [MIMIC-CXR](https://physionet.org/content/mimic-cxr/)

---

## ğŸ“ Project Structure

```
llm-chest-xray/
â”œâ”€â”€ Llama_3_2_Vision_Finetuning_Unsloth_Xrays.ipynb   # Main notebook
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/                                          # Training images
â”‚   â”œâ”€â”€ val/                                            # Validation images
â”‚   â””â”€â”€ dataset.json                                    # Labels / reports
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ checkpoint-*/                                   # LoRA checkpoints
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Base Model | LLaMA 3.2 Vision (Meta) |
| Fine-tuning | Unsloth â€” 2Ã— faster, 60% less VRAM |
| Adaptation | PEFT / LoRA / QLoRA (Hugging Face) |
| Framework | PyTorch + Accelerate |
| Pipelines | Hugging Face Transformers |
| Datasets | ChestX-ray14, MIMIC-CXR |

---

## ğŸ”® Roadmap

| Version | Feature |
|---------|---------|
| v1.0 | Fine-tuning notebook â€” classification + captioning âœ… |
| v1.1 | Full MIMIC-CXR report generation pipeline |
| v1.2 | Multi-label pathology classification |
| v2.0 | Flask web app â€” upload X-ray, get AI report |
| v2.1 | DICOM (.dcm) file support |
| v2.2 | GradCAM visual explanation overlays |
| v3.0 | Benchmark: LLaMA vs BioViL vs CheXagent vs MedPaLM |
| v3.1 | RLHF with radiologist feedback |

---

## âš ï¸ Disclaimer

This project is **strictly for research and educational purposes**.
It is **not validated for clinical use** and must **not** be used to make or influence
medical decisions. All outputs require review by a qualified radiologist or physician.

---

## ğŸ“– Acknowledgements

- [UnslothAI](https://github.com/unslothai/unsloth) â€” efficient fine-tuning framework
- [Hugging Face](https://huggingface.co) â€” Transformers, PEFT, Datasets
- [NIH Clinical Center](https://nihcc.app.box.com/v/ChestXray-NIHCC) â€” ChestX-ray14
- [PhysioNet / MIT](https://physionet.org/content/mimic-cxr/) â€” MIMIC-CXR
- [Meta AI](https://ai.meta.com/llama/) â€” LLaMA 3.2 Vision

---

## ğŸ¤ Open to Collaboration

Looking to connect with:
- ğŸ¥ Radiologists interested in AI-assisted reporting
- ğŸ§¬ Medical AI researchers working on foundation models
- ğŸ¤— VLM researchers pushing multimodal medical AI
- ğŸš€ Healthcare startups building clinical AI products
- ğŸŒ Global health technologists expanding diagnostic access

**Let's build medical AI that actually helps people.**

---

## ğŸ“œ License

MIT License â€” open for research and educational use with attribution.

---

â­ Star Â· ğŸ´ Fork Â· ğŸ’¬ Contribute Â· ğŸ“¤ Share

```






